{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('tiny_shakespear.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "892314\n",
      "223079\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "char_to_int = dict()\n",
    "int_to_char = dict()\n",
    "\n",
    "for i in range (len(chars)):\n",
    "    char_to_int[chars[i]] = i\n",
    "    int_to_char[i] = chars[i]\n",
    "\n",
    "print(char_to_int)\n",
    "print(int_to_char)\n",
    "\n",
    "data = []\n",
    "for char in text:\n",
    "    data.append(char_to_int[char])\n",
    "\n",
    "data = torch.tensor(data)\n",
    "split_idx = int(len(data)*0.8)\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([33]) target: 25\n",
      "input: tensor([33, 25]) target: 14\n",
      "input: tensor([33, 25, 14]) target: 17\n",
      "input: tensor([33, 25, 14, 17]) target: 30\n",
      "input: tensor([33, 25, 14, 17, 30]) target: 24\n",
      "input: tensor([33, 25, 14, 17, 30, 24]) target: 13\n",
      "input: tensor([33, 25, 14, 17, 30, 24, 13]) target: 26\n",
      "input: tensor([33, 25, 14, 17, 30, 24, 13, 26]) target: 16\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 4 \n",
    "context_length = 8\n",
    "\n",
    "def get_batch(n = batch_size, type = \"train\"):\n",
    "    if type == \"train\":\n",
    "        data = train_data\n",
    "\n",
    "    elif type == \"val\":\n",
    "        data = val_data\n",
    "\n",
    "    data_len = len(data) - context_length\n",
    "    ix = torch.randint(data_len, size=(n,))\n",
    "\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_length] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch()\n",
    "\n",
    "for i in range(context_length):\n",
    "    print(f\"input: {x[0,:i+1]} target: {y[0,i]}\")\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, context_length = 8, vocab_embed_dim = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.vocab_embed_dim = vocab_embed_dim\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.vocab_embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(self.context_length, self.vocab_embed_dim)\n",
    "\n",
    "        self.l1 = nn.Linear(32, self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        print(x.shape)\n",
    "        \n",
    "        # (B, T, vocab_embed) + (T, vocab_embed)\n",
    "        tok_emb = self.token_embedding_table(x)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(self.context_length)) \n",
    "        x_enc =  tok_emb + pos_emb\n",
    "        print(x_enc.shape)\n",
    "        logits = self.l1(x_enc)\n",
    "\n",
    "        if y == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #Batch, Time, Classes\n",
    "            logits_flat = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flat, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, prompt, max_response_len):\n",
    "        for _ in range(max_response_len):\n",
    "            logits, loss = self.forward(prompt)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            prompt = torch.cat((prompt, next_token), dim=1)\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8, 32])\n",
      "torch.Size([4, 8, 65])\n",
      "tensor(4.4324, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = m.forward(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 65])\n"
     ]
    }
   ],
   "source": [
    "logits, loss = m.forward(torch.zeros((1,1), dtype=torch.long))\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se, gos bene thear ou O,\n",
      "I surulllee pp, bepade acow, hese lld ndyousoondre y bureerars se irade,\n",
      "Fi\n"
     ]
    }
   ],
   "source": [
    "response = m.generate(prompt= torch.zeros((1,1), dtype=torch.long), max_response_len=100)\n",
    "\n",
    "data = ''.join([int_to_char[v.item()] for v in response[0]])\n",
    "print(data)\n",
    "\n",
    "\n",
    "#Pure randomness, but structure can be found... simply by looking at probabilites, conditioned on state..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001, amsgrad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.630314588546753\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    X, Y = get_batch()\n",
    "    logits, loss = m.forward(X, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical trick for efficent self-attention (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 8])\n",
      "torch.Size([1, 8, 16])\n",
      "torch.Size([1, 8, 16])\n",
      "torch.Size([1, 8, 8])\n",
      "tensor([[-0.5249,  0.7228,  1.5818,  3.6537, -0.3164, -0.4668,  0.2571, -0.6779],\n",
      "        [-1.9168, -0.0153, -0.6245, -1.5969,  1.4872, -0.7036, -0.4554,  0.4036],\n",
      "        [-1.1580,  1.6528,  0.1651, -0.4505,  1.2689, -1.2856, -0.1286, -1.5790],\n",
      "        [ 0.1870,  1.7720,  2.0020,  0.9950,  0.7860, -1.0472, -1.0654, -2.0595],\n",
      "        [-2.0954,  1.1104,  0.8074, -3.3844,  0.6437, -0.6074, -0.3781,  0.4568],\n",
      "        [-0.4413,  1.6315,  0.4316,  0.0982, -0.4590,  0.1698,  1.1595, -0.4861],\n",
      "        [-0.1293,  0.6867,  1.0631,  2.9071, -1.3107,  1.2985,  0.6031, -1.0113],\n",
      "        [-1.5245,  2.3170,  0.8273, -3.2456,  2.0506, -2.5999,  0.6020,  1.0496]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.1824,  0.6020,  0.0880,  0.0272,  0.1562,  0.4874,  0.0839, -0.8607,\n",
      "         0.0247,  0.6225,  0.2833,  0.3509, -0.0809,  0.5528,  0.9446,  0.6471],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# How to extend to multi-dimensions?\n",
    "# https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "\n",
    "B, T, C = 1, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "#encode node into key...\n",
    "key = nn.Linear(32, head_size, bias=False)\n",
    "\n",
    "#encode node to query. Dimension must match key dim.\n",
    "query = nn.Linear(32, head_size, bias=False)\n",
    "\n",
    "#Determine what information to share (x -> private information, v -> public information)\n",
    "\n",
    "value = nn.Linear(32, head_size, bias=False)\n",
    "\n",
    "# Each token, which right now is represented by a C-dimensional vector we are now going to encode it into a key/query. We do this by taking a linear sum of it's values\n",
    "# That key represents what each token \"has\", and the query is what it's looking for. \n",
    "# It's a soft search in contious space, so if things are not exactly the same they can still show up.\n",
    "\n",
    "K = key(x)\n",
    "Q = query(x)\n",
    "V = value(x)\n",
    "\n",
    "\n",
    "print(Q.transpose(-2,-1).shape)\n",
    "\n",
    "print(K.shape)\n",
    "print(Q.shape)\n",
    "A = K @ Q.transpose(-2,-1)\n",
    "print(A.shape)\n",
    "print(A[0])\n",
    "tril = torch.tril(torch.ones((B,T,T)))\n",
    "A = A.masked_fill(tril == 0, float('-inf'))\n",
    "A = F.softmax(A, dim=-1)\n",
    "\n",
    "output = A @ V\n",
    "\n",
    "print(A[0][0])\n",
    "print(output[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7836)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(11)\n",
    "\n",
    "x @ x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does uniform averaging work?\n",
    "\n",
    "Lets say want to determine the sentiment of document. Actually just a 1 dimensional vector would be enough.\n",
    "\n",
    "Enocde each word with a [+1] if positive, and [-1] if negative. You can then average all vectors together. the more positive it is, the better! Doesn't depend on document length, but rather of the percent of words that are positive vs negative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we add the position encoding?**\n",
    "\n",
    "https://datascience.stackexchange.com/questions/55901/in-a-transformer-model-why-does-one-sum-positional-encoding-to-the-embedding-ra\n",
    "\n",
    "Example:\n",
    "\n",
    "in a single dimension a key could be 1 and the query could be /1. actually hard to make it pluck out a single positional encoding...\n",
    "\n",
    "1 / -1 + 1\n",
    "\n",
    "2D vector, I can have basically infite going around a circle, and then I can have another pointing in the same direction, and it will be the max!...\n",
    "\n",
    "**Common defs**\n",
    "\n",
    "- in Encoder blocks we typically remove the masked attention, to allow all tokens to communicate with each other\n",
    "- in decoder blocks, we have the masked attention, beacuse the future is basically unknown?\n",
    "- self-attention is when the same nodes produce the key, queries, values\n",
    "- cross attention is when one set of nodes produce the keys and values, and another the queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9817)\n",
      "tensor(0.9941)\n",
      "tensor(29.5017)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B, T, 16)\n",
    "q = torch.randn(B, T, 16)\n",
    "\n",
    "a = k @ q.transpose(-2,-1)\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(a.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
