{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big paradigm shift has happened recently.\n",
    "\n",
    "Previously I was thinking about robotics in term of a policy that maps from state -> action.\n",
    "\n",
    "Think/plan/act\n",
    "sense/process/action\n",
    "\n",
    "# Subsumption architecture\n",
    "\n",
    "Goal was to make robots much more reactive. Brooks was impressed by how intelligent the little turtle with a simple light sensor could be (see bug algorithims).\n",
    "\n",
    "The prevaling approach at the time was to try to build a world model, plan in that world model, and then execute. The issue with this, is that building an accurate model of the world is hard! There is much we cannot predict (a car coming from around a corner). If the world model is wrong, our actions will be wrong, and our behaviour will look silly.\n",
    "\n",
    "For me this became clear in the Audi Autonmous Cup. We had a architecture based on accurate localization. Ultimatetly this wasn't very robust, as any error in the localization would mean the car would veer off the road. A \"more\" reactive approach, would be just look at the world as it is, and see the car lines. The world it's self is the best model!\n",
    "\n",
    "I am reminded of Don Normans points on information in the world vs in the head, there is so much structure in the world that we can utlise so we don't need to hold it all in our heads. \n",
    "\n",
    "Of course purely reactive architectures have short comings as well, You can make reactive architectures incoperate planning, via a Dyna like architecture, alpha go/ muscle memory. Ensures real time run speed....\n",
    "\n",
    "- Goal was to mimic the subconcious parts of the mind (system 1) which I think is what does alot of the heavy lifting!\n",
    "- Just open your eyes! no need to guess.\n",
    "\n",
    "Big idea of emergence, how complex behaviour can come from a simple set of \"rules\". Game of life, physics, simple ideas, a couple colors, endless options...\n",
    "\n",
    "# Sequence Modelling\n",
    "\n",
    "The lens you look at a problem can change the approach.\n",
    "\n",
    "The action we take at any moment, is not just based on the most recent observation, but instead of a series of based observations and actions.\n",
    "\n",
    "Something that has bugged me for a while is that the neural network has no sense of \"time\". It doesn't matter when the input is fed in the answer will be the same! The fact that things are sequentially ordered in time is a huge deal and that can be taken advantage off.\n",
    "\n",
    "Robotics perhaps should look alot more like lanugaue modelling...\n",
    "\n",
    "Single letters - abaskdujqhweilqjkwhdajsdkl are muscle twiches. \n",
    "Words are complete actions - reach forward, grasp\n",
    "We can combine thoose words, which may be of differents lengths to create sentances. And we can combine thoose sentences to create paragraphs, and we combine paragraphs to create chapters, and we combine chapters to makes books, and we combine books to make libraries, and we combine libraries to create knowledge.\n",
    "\n",
    "\n",
    "This heirachicaly structure that we see in vision, how to close the gap between high level thinking and the low level muscle control.... At some point it needs to happen...\n",
    "\n",
    "A key difference though is that we require interaction with the enviroment...\n",
    "\n",
    "It's like a conversation with another agent who is the enviroment...\n",
    "\n",
    "The world is categoried best perhaps as continum? regression the number of classes just exploed... For every perumation of the world..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have a dataset that is a series of \"correct\" sequences \n",
    "\n",
    "In RL, we can have a dataset with \"correct\" and \"incorrect\" sequences. \n",
    "\n",
    "Now in both cases, we want to build a generative model, ie one that can generate sequences, and we want to build that generative model such that it maximises the probability of correct sequences, and minises the probability of incorrect sequences. \n",
    "\n",
    "To test the generative model, we can run it, and we would hope that the sequences look alot like the correct sesquences.\n",
    "Actually, given the same starting point, it really be identical to one of the sequences in the training set.\n",
    "\n",
    "We can also split into train and validation, we can make sure that probability of the correct val sequenecs in high, and prob of incorreect sequences is low. This will be a good check to verify, that the model is not just memorising the dataset (which to some extent always happens, but the more diverse the data the harder it becomes) and is exactly \"understanding\" the underlying structure.\n",
    "\n",
    "Over time the model is correctly predict the sequences. but in life, we have the moving goal post. What used to be good is no longer good, what is bad is still bad. By still keeping some options for exploration open, every so often, we may take a random action, and we may find that it performs better. We are not just doing random search, we are being guided by the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "v1, I want to create a model that can generate good names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "# Split into input/out put targets\n",
    "# Split into train/val\n",
    "\n",
    "I think you can build alot of intutions and get alot of practice by working with sequential data, text even if it's not exactly of the form that you will be using...\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
