{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnv:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.probs = np.random.rand(n_arms)  # Random probabilities for each arm\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= self.n_arms:\n",
    "            raise ValueError(\"Invalid action. Must be between 0 and n_arms - 1.\")\n",
    "        \n",
    "        win_prob = self.probs[action]  # Get probability of winning for the selected arm\n",
    "        reward = 1 if np.random.rand() < win_prob else 0  # Sample a random value\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54232661]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 1\n",
    "env = BanditEnv(n_arms)\n",
    "reward_table = np.zeros((n_arms))\n",
    "attempt_table = np.zeros((n_arms))\n",
    "\n",
    "print(env.probs)\n",
    "\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    action = np.random.randint(0, n_arms)\n",
    "    # print(action)\n",
    "        \n",
    "    reward = env.step(action)\n",
    "    # print(reward)\n",
    "\n",
    "    reward_table[action] = reward_table[action] + reward\n",
    "    attempt_table[action] = attempt_table[action] + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[791.]\n",
      "[1500.]\n",
      "[0.52733333]\n"
     ]
    }
   ],
   "source": [
    "print(reward_table)\n",
    "print(attempt_table)\n",
    "\n",
    "estimate_value = reward_table/attempt_table\n",
    "\n",
    "print(estimate_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder how long it will take to converege to a certain value? I bet you could say something about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super beutiful trick here to do it \"online\"\n",
    "\n",
    "Instead of keeping to tallies...\n",
    "\n",
    "Do more of a weighted average...\n",
    "\n",
    "if fully update each time...\n",
    "\n",
    "value = last_reward\n",
    "\n",
    "but.. if you add a small step in there...\n",
    "\n",
    "value = curr_value + 0.1 * curr_reward -> the impact of previous rewards exponentially decays... you can write it out and you will it's a series!\n",
    "\n",
    "value = curr_value + 1/n * curr_reward -> the impact of previous rewards decays and this value convereges. Not good for if the state is changing though.\n",
    "\n",
    "Is that equivlent to taking the average at each time step? I feel not entirely. beaucse you weigh the eariler rewards more than the later rewards... I guess that is how first impressions work though :)\n",
    "\n",
    "Ok you can do it! (see my notebook)\n",
    "\n",
    "new_value = curr_value * (n-1/n) + curr_reward/n\n",
    "\n",
    "This just requires you to remember the current value and the number of times you tried it\n",
    "\n",
    "Using the other way.\n",
    "\n",
    "I still need to remeber the number of times I tried it... but I need to hold this very large value of total rewards to date, and then I calculate the value...\n",
    "\n",
    "Doing the online version. I don't need to keep track of total rewards to date. I just remember the value. This is more biologically plausiable as I don't remember every single icecream I had, but I do remember some overall sense of goodness... and I know how to update it...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24500936]\n"
     ]
    }
   ],
   "source": [
    "n_arms = 1\n",
    "env = BanditEnv(n_arms)\n",
    "value_table = np.zeros((n_arms))\n",
    "attempt_table = np.zeros((n_arms))\n",
    "\n",
    "print(env.probs)\n",
    "\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    action = np.random.randint(0, n_arms)\n",
    "    # print(action)\n",
    "        \n",
    "    reward = env.step(action)\n",
    "    # print(reward)\n",
    "    n_attempt = attempt_table[action] + 1\n",
    "    value_table[action] = value_table[action] * (n_attempt-1)/n_attempt + reward/n_attempt\n",
    "    attempt_table[action] =  n_attempt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n"
     ]
    }
   ],
   "source": [
    "print(value_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
